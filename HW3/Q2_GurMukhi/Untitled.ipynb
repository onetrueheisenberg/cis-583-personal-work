{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4575ef2-72d7-4d77-bf6f-3e4d13d79554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset init, data_dir: /Users/sundarasubramanian/yoyo/CIS-583/HW3/Q2_GurMukhi/GurNum/train/\n",
      "  Processing class: 9\n",
      "  Processing class: 0\n",
      "  Processing class: 7\n",
      "  Processing class: 6\n",
      "  Processing class: 1\n",
      "  Processing class: 8\n",
      "  Processing class: 4\n",
      "  Processing class: 3\n",
      "  Processing class: 2\n",
      "  Processing class: 5\n",
      "  Total images loaded: 1000\n",
      "Dataset init, data_dir: /Users/sundarasubramanian/yoyo/CIS-583/HW3/Q2_GurMukhi/GurNum/val/\n",
      "  Processing class: 9\n",
      "  Processing class: 0\n",
      "  Processing class: 7\n",
      "  Processing class: 6\n",
      "  Processing class: 1\n",
      "  Processing class: 8\n",
      "  Processing class: 4\n",
      "  Processing class: 3\n",
      "  Processing class: 2\n",
      "  Processing class: 5\n",
      "  Total images loaded: 178\n",
      "Train dataset size: 1000\n",
      "Test dataset size: 178\n",
      "Running Gradient Checking for initial model (no regularization)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GurmukhiDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        print(f\"Dataset init, data_dir: {data_dir}\")\n",
    "        self.data_dir = data_dir\n",
    "        self.image_set = []\n",
    "        self.transform = transform\n",
    "        for class_name in os.listdir(data_dir):\n",
    "            class_path = os.path.join(self.data_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                print(f\"  Processing class: {class_name}\")\n",
    "                for img_file in os.listdir(class_path):\n",
    "                    self.image_set.append((os.path.join(class_path, img_file), int(class_name)))\n",
    "        print(f\"  Total images loaded: {len(self.image_set)}\")\n",
    "    def __len__(self):\n",
    "        return len(self.image_set)\n",
    "    def __getitem__(self, index):\n",
    "        image_name, label = self.image_set[index]\n",
    "        image = Image.open(image_name).convert('L')\n",
    "        label = int(os.path.basename(os.path.dirname(image_name)))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "root_dir = '/Users/sundarasubramanian/yoyo/CIS-583/HW3/Q2_GurMukhi/GurNum/' # Replace with your actual path\n",
    "transform = transforms.Compose([transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = GurmukhiDataset(data_dir = f\"{root_dir}train/\", transform = transform)\n",
    "test_dataset = GurmukhiDataset(data_dir = f\"{root_dir}val/\", transform = transform)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False) # Shuffle False for consistent test loss\n",
    "\n",
    "def l1_regularization(model, lambda_l1=0.001):\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    return lambda_l1 * l1_norm\n",
    "\n",
    "def l2_regularization(model, lambda_l2=0.001):\n",
    "    l2_norm = sum((p ** 2).sum() for p in model.parameters())\n",
    "    return lambda_l2 * l2_norm\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(NN, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(self.relu(self.conv1(x)))\n",
    "        x = self.max_pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout_manual(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def dropout_manual(self, x):\n",
    "        if self.training:\n",
    "            mask = (torch.rand_like(x) > self.dropout_prob).float()\n",
    "            x = x * mask / (1 - self.dropout_prob)\n",
    "        return x\n",
    "\n",
    "def gradient_checking(model, criterion, X, y, epsilon=1e-5):\n",
    "    grad_diffs = []\n",
    "    param_names = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    X, y = X.to(torch.float32), y.to(torch.long)\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param_data = param.data.clone()\n",
    "            grad_approx = torch.zeros_like(param)\n",
    "\n",
    "            for i in range(param.numel()):\n",
    "                param.data.view(-1)[i] += epsilon\n",
    "                loss1 = criterion(model(X), y).item()\n",
    "\n",
    "                param.data.view(-1)[i] -= 2 * epsilon\n",
    "                loss2 = criterion(model(X), y).item()\n",
    "\n",
    "                grad_approx.view(-1)[i] = (loss1 - loss2) / (2 * epsilon)\n",
    "                param.data = param_data\n",
    "\n",
    "            if param.grad is not None:\n",
    "                grad_diff = torch.norm(param.grad - grad_approx) / (torch.norm(param.grad + grad_approx) + 1e-7)\n",
    "                grad_diffs.append(grad_diff.item())\n",
    "                param_names.append(name)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(param_names, grad_diffs, color='skyblue')\n",
    "    plt.xlabel(\"Gradient Difference\")\n",
    "    plt.ylabel(\"Model Parameters\")\n",
    "    plt.title(\"Gradient Checking Differences for Each Parameter\")\n",
    "    plt.show()\n",
    "\n",
    "def train_and_evaluate(model, optimizer, criterion, train_loader, test_loader, epochs, regularization='none', lambda_l1=0.001, lambda_l2=0.001):\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            if regularization == 'l1':\n",
    "                loss += l1_regularization(model, lambda_l1)\n",
    "            elif regularization == 'l2':\n",
    "                loss += l2_regularization(model, lambda_l2)\n",
    "            elif regularization == 'l1_l2':\n",
    "                loss += l1_regularization(model, lambda_l1) + l2_regularization(model, lambda_l2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch: {epoch + 1} / {epochs}; Train Loss ({regularization}): {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f\"Epoch: {epoch + 1} / {epochs}; Test Loss ({regularization}): {avg_test_loss:.4f}; Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses, accuracy\n",
    "\n",
    "# Hyperparameters and settings\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.01\n",
    "epochs = 10\n",
    "lambda_l1 = 0.0005\n",
    "lambda_l2 = 0.001\n",
    "dropout_prob = 0.5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Configurations to train and compare\n",
    "regularization_types = ['none', 'l1', 'l2', 'dropout', 'l1_l2_dropout']\n",
    "all_train_losses = {}\n",
    "all_test_losses = {}\n",
    "all_accuracies = {}\n",
    "\n",
    "# --- Gradient Checking (Run once before training) ---\n",
    "print(\"Running Gradient Checking for initial model (no regularization)...\")\n",
    "initial_model = NN(dropout_prob=dropout_prob) # Model with dropout for gradient check\n",
    "X_sample, y_sample = next(iter(train_loader))\n",
    "gradient_checking(initial_model, criterion, X_sample, y_sample)\n",
    "print(\"Gradient Checking Completed.\\n\")\n",
    "\n",
    "for regularization_type in regularization_types:\n",
    "    print(f\"\\n--- Training with Regularization: {regularization_type} ---\")\n",
    "    model = NN(dropout_prob=dropout_prob)\n",
    "\n",
    "    # Optimizer - Adam with weight decay (L2 built-in, but we are comparing manual L2)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0 if regularization_type != 'l2_pytorch' else weight_decay) # Use weight_decay for pytorch L2 only\n",
    "\n",
    "    if regularization_type == 'dropout':\n",
    "        # Train with manual dropout already in the NN class.\n",
    "        pass # Dropout is handled within the NN class itself\n",
    "\n",
    "    train_losses, test_losses, accuracy = train_and_evaluate(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        epochs=epochs,\n",
    "        regularization=regularization_type if regularization_type in ['l1', 'l2', 'l1_l2'] else 'none', # Apply L1, L2, L1_L2 only when specified\n",
    "        lambda_l1=lambda_l1,\n",
    "        lambda_l2=lambda_l2\n",
    "    )\n",
    "\n",
    "    all_train_losses[regularization_type] = train_losses\n",
    "    all_test_losses[regularization_type] = test_losses\n",
    "    all_accuracies[regularization_type] = accuracy\n",
    "\n",
    "# --- Plotting Loss Curves ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "for regularization_type in regularization_types:\n",
    "    plt.plot(range(1, epochs + 1), all_train_losses[regularization_type], label=f'Train Loss ({regularization_type})')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for regularization_type in regularization_types:\n",
    "    plt.plot(range(1, epochs + 1), all_test_losses[regularization_type], label=f'Test Loss ({regularization_type})')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Test Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Print Accuracies ---\n",
    "print(\"\\n--- Final Accuracies ---\")\n",
    "for regularization_type in regularization_types:\n",
    "    print(f\"Accuracy ({regularization_type}): {all_accuracies[regularization_type]:.4f}\")\n",
    "\n",
    "# --- Performance Comparison and Reasons ---\n",
    "print(\"\\n--- Performance Comparison and Reasons ---\")\n",
    "print(\"Comparison based on Training Loss, Test Loss and Test Accuracy after 10 epochs:\")\n",
    "print(\"Observations and Reasons will be detailed here based on the plots and accuracies.\")\n",
    "print(\"\"\"\n",
    "**Observations and Reasons (Example - Replace with your actual analysis after running):**\n",
    "\n",
    "* **No Regularization:** Serves as the baseline. May overfit to the training data, leading to lower test accuracy compared to regularized models.\n",
    "* **L1 Regularization:**  Tends to drive less important feature weights to exactly zero, leading to feature selection and potentially simpler models. Might improve generalization and reduce overfitting compared to no regularization.\n",
    "* **L2 Regularization:**  Penalizes large weights, encouraging weights to be small but not exactly zero.  Helps to reduce overfitting and improve generalization by making the model less sensitive to individual features of the training data.\n",
    "* **Dropout:** Randomly drops out neurons during training, preventing neurons from co-adapting too much and forcing the network to learn more robust features. Effective in reducing overfitting.\n",
    "* **L1+L2+Dropout:** Combines the benefits of all three regularization techniques. Might provide the best generalization but also might be harder to tune the regularization strengths (lambdas and dropout probability).\n",
    "\n",
    "**Expected Outcomes (Hypothetical - Replace with your actual outcomes):**\n",
    "\n",
    "* Models with L1, L2, and Dropout are expected to have lower test loss and higher test accuracy compared to the model with no regularization, indicating better generalization.\n",
    "* The combination of L1, L2, and Dropout might offer the best performance, but it depends on the dataset and hyperparameters.\n",
    "* L1 might lead to sparser weights, which can be an advantage in terms of model interpretability and potentially faster inference in some hardware.\n",
    "* L2 might lead to smoother weight distributions and stable training.\n",
    "* Dropout might be very effective in preventing overfitting, especially in larger networks.\n",
    "\n",
    "**Further Analysis:**\n",
    "\n",
    "* You can further analyze by varying the regularization strengths (lambda_l1, lambda_l2, dropout_prob) and observing their impact on performance.\n",
    "* Comparing the magnitude of weights in models trained with L1 vs. L2 regularization can also provide insights.\n",
    "* Observing the training and test loss curves can indicate if overfitting is occurring and how well each regularization technique mitigates it.\n",
    "\n",
    "**Remember to replace the \"Example\" and \"Hypothetical\" sections with your actual observations and analysis after running the code.**\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1ea6d-03c4-46ef-abaa-d920db847689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
