{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41328d84-b9d4-4843-bd6b-6418a0d26bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.9948, Test ROC AUC: 0.4898\n",
      "Epoch 2/200, Loss: 0.9940, Test ROC AUC: 0.5014\n",
      "Epoch 3/200, Loss: 0.9871, Test ROC AUC: 0.5129\n",
      "Epoch 4/200, Loss: 0.9852, Test ROC AUC: 0.5224\n",
      "Epoch 5/200, Loss: 0.9856, Test ROC AUC: 0.5310\n",
      "Epoch 6/200, Loss: 0.9948, Test ROC AUC: 0.5382\n",
      "Epoch 7/200, Loss: 0.9829, Test ROC AUC: 0.5440\n",
      "Epoch 8/200, Loss: 0.9713, Test ROC AUC: 0.5481\n",
      "Epoch 9/200, Loss: 0.9918, Test ROC AUC: 0.5523\n",
      "Epoch 10/200, Loss: 0.9765, Test ROC AUC: 0.5569\n",
      "Epoch 11/200, Loss: 1.0006, Test ROC AUC: 0.5610\n",
      "Epoch 12/200, Loss: 1.0003, Test ROC AUC: 0.5646\n",
      "Epoch 13/200, Loss: 0.9410, Test ROC AUC: 0.5680\n",
      "Epoch 14/200, Loss: 0.9628, Test ROC AUC: 0.5714\n",
      "Epoch 15/200, Loss: 0.9326, Test ROC AUC: 0.5741\n",
      "Epoch 16/200, Loss: 0.9345, Test ROC AUC: 0.5767\n",
      "Epoch 17/200, Loss: 0.9479, Test ROC AUC: 0.5792\n",
      "Epoch 18/200, Loss: 0.9094, Test ROC AUC: 0.5814\n",
      "Epoch 19/200, Loss: 0.9754, Test ROC AUC: 0.5836\n",
      "Epoch 20/200, Loss: 0.9576, Test ROC AUC: 0.5856\n",
      "Epoch 21/200, Loss: 0.9622, Test ROC AUC: 0.5880\n",
      "Epoch 22/200, Loss: 0.9558, Test ROC AUC: 0.5903\n",
      "Epoch 23/200, Loss: 0.9370, Test ROC AUC: 0.5927\n",
      "Epoch 24/200, Loss: 0.9049, Test ROC AUC: 0.5949\n",
      "Epoch 25/200, Loss: 0.8931, Test ROC AUC: 0.5969\n",
      "Epoch 26/200, Loss: 0.8918, Test ROC AUC: 0.5991\n",
      "Epoch 27/200, Loss: 0.8849, Test ROC AUC: 0.6010\n",
      "Epoch 28/200, Loss: 0.8974, Test ROC AUC: 0.6028\n",
      "Epoch 29/200, Loss: 0.9074, Test ROC AUC: 0.6047\n",
      "Epoch 30/200, Loss: 0.8266, Test ROC AUC: 0.6067\n",
      "Epoch 31/200, Loss: 0.8406, Test ROC AUC: 0.6084\n",
      "Epoch 32/200, Loss: 0.9138, Test ROC AUC: 0.6103\n",
      "Epoch 33/200, Loss: 0.8865, Test ROC AUC: 0.6124\n",
      "Epoch 34/200, Loss: 0.8331, Test ROC AUC: 0.6144\n",
      "Epoch 35/200, Loss: 0.8642, Test ROC AUC: 0.6165\n",
      "Epoch 36/200, Loss: 0.8373, Test ROC AUC: 0.6187\n",
      "Epoch 37/200, Loss: 0.8085, Test ROC AUC: 0.6207\n",
      "Epoch 38/200, Loss: 0.9147, Test ROC AUC: 0.6227\n",
      "Epoch 39/200, Loss: 0.8306, Test ROC AUC: 0.6244\n",
      "Epoch 40/200, Loss: 0.8726, Test ROC AUC: 0.6264\n",
      "Epoch 41/200, Loss: 0.8947, Test ROC AUC: 0.6284\n",
      "Epoch 42/200, Loss: 0.8457, Test ROC AUC: 0.6304\n",
      "Epoch 43/200, Loss: 0.7553, Test ROC AUC: 0.6325\n",
      "Epoch 44/200, Loss: 0.7362, Test ROC AUC: 0.6344\n",
      "Epoch 45/200, Loss: 0.8413, Test ROC AUC: 0.6364\n",
      "Epoch 46/200, Loss: 0.8925, Test ROC AUC: 0.6382\n",
      "Epoch 47/200, Loss: 0.8568, Test ROC AUC: 0.6399\n",
      "Epoch 48/200, Loss: 0.7782, Test ROC AUC: 0.6416\n",
      "Epoch 49/200, Loss: 0.7085, Test ROC AUC: 0.6431\n",
      "Epoch 50/200, Loss: 0.8018, Test ROC AUC: 0.6447\n",
      "Epoch 51/200, Loss: 0.8472, Test ROC AUC: 0.6460\n",
      "Epoch 52/200, Loss: 0.7352, Test ROC AUC: 0.6473\n",
      "Epoch 53/200, Loss: 0.7778, Test ROC AUC: 0.6486\n",
      "Epoch 54/200, Loss: 0.7899, Test ROC AUC: 0.6497\n",
      "Epoch 55/200, Loss: 0.7425, Test ROC AUC: 0.6508\n",
      "Epoch 56/200, Loss: 0.8334, Test ROC AUC: 0.6520\n",
      "Epoch 57/200, Loss: 0.8316, Test ROC AUC: 0.6531\n",
      "Epoch 58/200, Loss: 0.7698, Test ROC AUC: 0.6542\n",
      "Epoch 59/200, Loss: 0.8583, Test ROC AUC: 0.6553\n",
      "Epoch 60/200, Loss: 0.7949, Test ROC AUC: 0.6564\n",
      "Epoch 61/200, Loss: 0.8243, Test ROC AUC: 0.6576\n",
      "Epoch 62/200, Loss: 0.7094, Test ROC AUC: 0.6586\n",
      "Epoch 63/200, Loss: 0.6853, Test ROC AUC: 0.6596\n",
      "Epoch 64/200, Loss: 0.6155, Test ROC AUC: 0.6606\n",
      "Epoch 65/200, Loss: 0.8423, Test ROC AUC: 0.6615\n",
      "Epoch 66/200, Loss: 0.7883, Test ROC AUC: 0.6623\n",
      "Epoch 67/200, Loss: 0.8553, Test ROC AUC: 0.6632\n",
      "Epoch 68/200, Loss: 0.7936, Test ROC AUC: 0.6641\n",
      "Epoch 69/200, Loss: 0.7049, Test ROC AUC: 0.6651\n",
      "Epoch 70/200, Loss: 0.7407, Test ROC AUC: 0.6660\n",
      "Epoch 71/200, Loss: 0.7169, Test ROC AUC: 0.6668\n",
      "Epoch 72/200, Loss: 0.7555, Test ROC AUC: 0.6677\n",
      "Epoch 73/200, Loss: 0.8223, Test ROC AUC: 0.6685\n",
      "Epoch 74/200, Loss: 0.7165, Test ROC AUC: 0.6694\n",
      "Epoch 75/200, Loss: 0.7276, Test ROC AUC: 0.6704\n",
      "Epoch 76/200, Loss: 0.7983, Test ROC AUC: 0.6716\n",
      "Epoch 77/200, Loss: 0.8206, Test ROC AUC: 0.6728\n",
      "Epoch 78/200, Loss: 0.7547, Test ROC AUC: 0.6740\n",
      "Epoch 79/200, Loss: 0.7818, Test ROC AUC: 0.6752\n",
      "Epoch 80/200, Loss: 0.7189, Test ROC AUC: 0.6763\n",
      "Epoch 81/200, Loss: 0.7044, Test ROC AUC: 0.6774\n",
      "Epoch 82/200, Loss: 0.7709, Test ROC AUC: 0.6785\n",
      "Epoch 83/200, Loss: 0.7003, Test ROC AUC: 0.6795\n",
      "Epoch 84/200, Loss: 0.6366, Test ROC AUC: 0.6803\n",
      "Epoch 85/200, Loss: 0.8618, Test ROC AUC: 0.6812\n",
      "Epoch 86/200, Loss: 0.7843, Test ROC AUC: 0.6823\n",
      "Epoch 87/200, Loss: 0.6714, Test ROC AUC: 0.6834\n",
      "Epoch 88/200, Loss: 0.7087, Test ROC AUC: 0.6846\n",
      "Epoch 89/200, Loss: 0.6798, Test ROC AUC: 0.6860\n",
      "Epoch 90/200, Loss: 0.6761, Test ROC AUC: 0.6871\n",
      "Epoch 91/200, Loss: 0.7090, Test ROC AUC: 0.6883\n",
      "Epoch 92/200, Loss: 0.7930, Test ROC AUC: 0.6896\n",
      "Epoch 93/200, Loss: 0.9018, Test ROC AUC: 0.6909\n",
      "Epoch 94/200, Loss: 0.7138, Test ROC AUC: 0.6921\n",
      "Epoch 95/200, Loss: 0.7381, Test ROC AUC: 0.6934\n",
      "Epoch 96/200, Loss: 0.6889, Test ROC AUC: 0.6944\n",
      "Epoch 97/200, Loss: 0.7155, Test ROC AUC: 0.6956\n",
      "Epoch 98/200, Loss: 0.7195, Test ROC AUC: 0.6967\n",
      "Epoch 99/200, Loss: 0.6710, Test ROC AUC: 0.6977\n",
      "Epoch 100/200, Loss: 0.6731, Test ROC AUC: 0.6984\n",
      "Epoch 101/200, Loss: 0.6661, Test ROC AUC: 0.6992\n",
      "Epoch 102/200, Loss: 0.5517, Test ROC AUC: 0.6999\n",
      "Epoch 103/200, Loss: 0.7480, Test ROC AUC: 0.7007\n",
      "Epoch 104/200, Loss: 0.7171, Test ROC AUC: 0.7015\n",
      "Epoch 105/200, Loss: 0.6801, Test ROC AUC: 0.7025\n",
      "Epoch 106/200, Loss: 0.6468, Test ROC AUC: 0.7036\n",
      "Epoch 107/200, Loss: 0.7483, Test ROC AUC: 0.7045\n",
      "Epoch 108/200, Loss: 0.6354, Test ROC AUC: 0.7055\n",
      "Epoch 109/200, Loss: 0.7011, Test ROC AUC: 0.7064\n",
      "Epoch 110/200, Loss: 0.6181, Test ROC AUC: 0.7074\n",
      "Epoch 111/200, Loss: 0.5842, Test ROC AUC: 0.7082\n",
      "Epoch 112/200, Loss: 0.6140, Test ROC AUC: 0.7090\n",
      "Epoch 113/200, Loss: 0.6598, Test ROC AUC: 0.7099\n",
      "Epoch 114/200, Loss: 0.7524, Test ROC AUC: 0.7109\n",
      "Epoch 115/200, Loss: 0.5720, Test ROC AUC: 0.7118\n",
      "Epoch 116/200, Loss: 0.7002, Test ROC AUC: 0.7127\n",
      "Epoch 117/200, Loss: 0.6391, Test ROC AUC: 0.7136\n",
      "Epoch 118/200, Loss: 0.7410, Test ROC AUC: 0.7146\n",
      "Epoch 119/200, Loss: 0.7351, Test ROC AUC: 0.7157\n",
      "Epoch 120/200, Loss: 0.6381, Test ROC AUC: 0.7165\n",
      "Epoch 121/200, Loss: 0.6757, Test ROC AUC: 0.7175\n",
      "Epoch 122/200, Loss: 0.6181, Test ROC AUC: 0.7183\n",
      "Epoch 123/200, Loss: 0.6891, Test ROC AUC: 0.7192\n",
      "Epoch 124/200, Loss: 0.6761, Test ROC AUC: 0.7202\n",
      "Epoch 125/200, Loss: 0.6291, Test ROC AUC: 0.7210\n",
      "Epoch 126/200, Loss: 0.6101, Test ROC AUC: 0.7219\n",
      "Epoch 127/200, Loss: 0.6992, Test ROC AUC: 0.7226\n",
      "Epoch 128/200, Loss: 0.6455, Test ROC AUC: 0.7235\n",
      "Epoch 129/200, Loss: 0.6363, Test ROC AUC: 0.7242\n",
      "Epoch 130/200, Loss: 0.6126, Test ROC AUC: 0.7249\n",
      "Epoch 131/200, Loss: 0.5946, Test ROC AUC: 0.7257\n",
      "Epoch 132/200, Loss: 0.5341, Test ROC AUC: 0.7264\n",
      "Epoch 133/200, Loss: 0.6417, Test ROC AUC: 0.7272\n",
      "Epoch 134/200, Loss: 0.6688, Test ROC AUC: 0.7280\n",
      "Epoch 135/200, Loss: 0.5478, Test ROC AUC: 0.7287\n",
      "Epoch 136/200, Loss: 0.6083, Test ROC AUC: 0.7293\n",
      "Epoch 137/200, Loss: 0.6284, Test ROC AUC: 0.7298\n",
      "Epoch 138/200, Loss: 0.6323, Test ROC AUC: 0.7303\n",
      "Epoch 139/200, Loss: 0.6730, Test ROC AUC: 0.7308\n",
      "Epoch 140/200, Loss: 0.4637, Test ROC AUC: 0.7311\n",
      "Epoch 141/200, Loss: 0.6290, Test ROC AUC: 0.7315\n",
      "Epoch 142/200, Loss: 0.6758, Test ROC AUC: 0.7321\n",
      "Epoch 143/200, Loss: 0.4409, Test ROC AUC: 0.7327\n",
      "Epoch 144/200, Loss: 0.6394, Test ROC AUC: 0.7333\n",
      "Epoch 145/200, Loss: 0.6336, Test ROC AUC: 0.7341\n",
      "Epoch 146/200, Loss: 0.6965, Test ROC AUC: 0.7350\n",
      "Epoch 147/200, Loss: 0.5672, Test ROC AUC: 0.7357\n",
      "Epoch 148/200, Loss: 0.6689, Test ROC AUC: 0.7366\n",
      "Epoch 149/200, Loss: 0.6584, Test ROC AUC: 0.7375\n",
      "Epoch 150/200, Loss: 0.6779, Test ROC AUC: 0.7386\n",
      "Epoch 151/200, Loss: 0.6346, Test ROC AUC: 0.7394\n",
      "Epoch 152/200, Loss: 0.6600, Test ROC AUC: 0.7402\n",
      "Epoch 153/200, Loss: 0.5737, Test ROC AUC: 0.7407\n",
      "Epoch 154/200, Loss: 0.5231, Test ROC AUC: 0.7414\n",
      "Epoch 155/200, Loss: 0.5993, Test ROC AUC: 0.7420\n",
      "Epoch 156/200, Loss: 0.5583, Test ROC AUC: 0.7425\n",
      "Epoch 157/200, Loss: 0.5147, Test ROC AUC: 0.7431\n",
      "Epoch 158/200, Loss: 0.6117, Test ROC AUC: 0.7435\n",
      "Epoch 159/200, Loss: 0.5853, Test ROC AUC: 0.7441\n",
      "Epoch 160/200, Loss: 0.5314, Test ROC AUC: 0.7450\n",
      "Epoch 161/200, Loss: 0.5613, Test ROC AUC: 0.7457\n",
      "Epoch 162/200, Loss: 0.6388, Test ROC AUC: 0.7464\n",
      "Epoch 163/200, Loss: 0.6145, Test ROC AUC: 0.7471\n",
      "Epoch 164/200, Loss: 0.5985, Test ROC AUC: 0.7481\n",
      "Epoch 165/200, Loss: 0.5641, Test ROC AUC: 0.7490\n",
      "Epoch 166/200, Loss: 0.5547, Test ROC AUC: 0.7499\n",
      "Epoch 167/200, Loss: 0.6280, Test ROC AUC: 0.7507\n",
      "Epoch 168/200, Loss: 0.5248, Test ROC AUC: 0.7514\n",
      "Epoch 169/200, Loss: 0.5707, Test ROC AUC: 0.7523\n",
      "Epoch 170/200, Loss: 0.6176, Test ROC AUC: 0.7530\n",
      "Epoch 171/200, Loss: 0.4517, Test ROC AUC: 0.7537\n",
      "Epoch 172/200, Loss: 0.5803, Test ROC AUC: 0.7543\n",
      "Epoch 173/200, Loss: 0.5378, Test ROC AUC: 0.7547\n",
      "Epoch 174/200, Loss: 0.6707, Test ROC AUC: 0.7553\n",
      "Epoch 175/200, Loss: 0.3908, Test ROC AUC: 0.7558\n",
      "Epoch 176/200, Loss: 0.4971, Test ROC AUC: 0.7564\n",
      "Epoch 177/200, Loss: 0.6266, Test ROC AUC: 0.7570\n",
      "Epoch 178/200, Loss: 0.5266, Test ROC AUC: 0.7577\n",
      "Epoch 179/200, Loss: 0.6037, Test ROC AUC: 0.7586\n",
      "Epoch 180/200, Loss: 0.6040, Test ROC AUC: 0.7596\n",
      "Epoch 181/200, Loss: 0.5811, Test ROC AUC: 0.7604\n",
      "Epoch 182/200, Loss: 0.5412, Test ROC AUC: 0.7610\n",
      "Epoch 183/200, Loss: 0.5000, Test ROC AUC: 0.7614\n",
      "Epoch 184/200, Loss: 0.5033, Test ROC AUC: 0.7619\n",
      "Epoch 185/200, Loss: 0.6709, Test ROC AUC: 0.7624\n",
      "Epoch 186/200, Loss: 0.5641, Test ROC AUC: 0.7630\n",
      "Epoch 187/200, Loss: 0.6820, Test ROC AUC: 0.7636\n",
      "Epoch 188/200, Loss: 0.5600, Test ROC AUC: 0.7643\n",
      "Epoch 189/200, Loss: 0.5720, Test ROC AUC: 0.7648\n",
      "Epoch 190/200, Loss: 0.4553, Test ROC AUC: 0.7650\n",
      "Epoch 191/200, Loss: 0.4627, Test ROC AUC: 0.7652\n",
      "Epoch 192/200, Loss: 0.6131, Test ROC AUC: 0.7656\n",
      "Epoch 193/200, Loss: 0.5687, Test ROC AUC: 0.7657\n",
      "Epoch 194/200, Loss: 0.5295, Test ROC AUC: 0.7660\n",
      "Epoch 195/200, Loss: 0.5852, Test ROC AUC: 0.7663\n",
      "Epoch 196/200, Loss: 0.4294, Test ROC AUC: 0.7664\n",
      "Epoch 197/200, Loss: 0.5294, Test ROC AUC: 0.7665\n",
      "Epoch 198/200, Loss: 0.4975, Test ROC AUC: 0.7666\n",
      "Epoch 199/200, Loss: 0.6049, Test ROC AUC: 0.7668\n",
      "Epoch 200/200, Loss: 0.5332, Test ROC AUC: 0.7672\n",
      "Epoch 1/200, Loss: 1.0023, Test ROC AUC: 0.4979\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#########################################\n",
    "# Dataset loader: ML-100K integration\n",
    "#########################################\n",
    "def load_implicit_data():\n",
    "    # Load dataset\n",
    "    df = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    df = df.drop(columns=['timestamp'])\n",
    "    \n",
    "    # Adjust indices to start at 0\n",
    "    df['user_id'] -= 1\n",
    "    df['item_id'] -= 1\n",
    "    \n",
    "    # Split into training and testing sets.\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build a dictionary of user interactions.\n",
    "    def build_dict(df):\n",
    "        data = {}\n",
    "        for row in df.itertuples():\n",
    "            data.setdefault(row.user_id, []).append(row.item_id)\n",
    "        return data\n",
    "\n",
    "    train_data = build_dict(train_df)\n",
    "    test_data = build_dict(test_df)\n",
    "    \n",
    "    # Calculate number of users and items from the full dataset.\n",
    "    n_users = int(df['user_id'].max() + 1)\n",
    "    n_items = int(df['item_id'].max() + 1)\n",
    "    \n",
    "    return train_data, test_data, n_users, n_items\n",
    "\n",
    "#########################################\n",
    "# Sampling Triplets\n",
    "#########################################\n",
    "def sample_triplets(pos_data_train, n_users, n_items, num_samples=128):\n",
    "    \"\"\"\n",
    "    Uniformly samples triplets (user, positive item, negative item) from training data.\n",
    "    \"\"\"\n",
    "    user_ids = []\n",
    "    pos_item_ids = []\n",
    "    neg_item_ids = []\n",
    "    for _ in range(num_samples):\n",
    "        user = np.random.randint(0, n_users)\n",
    "        pos_items = pos_data_train.get(user, [])\n",
    "        if not pos_items:\n",
    "            continue\n",
    "        pos_item = np.random.choice(pos_items)\n",
    "        # Sample a negative item not in the user's positive set.\n",
    "        neg_item = np.random.randint(0, n_items)\n",
    "        while neg_item in pos_data_train.get(user, []):\n",
    "            neg_item = np.random.randint(0, n_items)\n",
    "        user_ids.append(user)\n",
    "        pos_item_ids.append(pos_item)\n",
    "        neg_item_ids.append(neg_item)\n",
    "    return (torch.LongTensor(user_ids),\n",
    "            torch.LongTensor(pos_item_ids),\n",
    "            torch.LongTensor(neg_item_ids))\n",
    "\n",
    "#########################################\n",
    "# Loss Function\n",
    "#########################################\n",
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, pos_score, neg_score):\n",
    "        # We want pos_score to be higher than neg_score by at least margin.\n",
    "        loss = F.relu(neg_score - pos_score + self.margin)\n",
    "        return loss.mean()\n",
    "\n",
    "#########################################\n",
    "# 1. Multi-Layer Perceptron (MLP)\n",
    "#########################################\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, n_hidden=1, hidden_size=64, dropout=0., l2_reg=None):\n",
    "        \"\"\"\n",
    "        Constructs an MLP.\n",
    "        - input_dim: Dimension of the input (e.g., user_dim + item_dim).\n",
    "        - n_hidden: Number of hidden layers.\n",
    "        - hidden_size: Hidden layer size.\n",
    "        - dropout: Dropout rate.\n",
    "        - l2_reg: (Not used here but can be passed to an optimizer for weight decay.)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        # Initial dropout layer\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        # Build hidden layers with ReLU activation.\n",
    "        for _ in range(n_hidden):\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "        # Final layer outputs 1 score with no activation.\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The output is squeezed so that it returns a scalar per sample.\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "#########################################\n",
    "# 2. DeepTripletModel\n",
    "#########################################\n",
    "class DeepTripletModel(nn.Module):\n",
    "    def __init__(self, n_users, n_items, user_dim=32, item_dim=64, margin=1.,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=None):\n",
    "        super().__init__()\n",
    "        # Embedding layers for users and items.\n",
    "        self.user_layer = nn.Embedding(n_users, user_dim)\n",
    "        self.item_layer = nn.Embedding(n_items, item_dim)\n",
    "        # The MLP processes the concatenated user-item features.\n",
    "        self.mlp = MLP(user_dim + item_dim, n_hidden, hidden_size, dropout, l2_reg)\n",
    "        self.margin_loss = MarginLoss(margin)\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, user_ids, pos_item_ids, neg_item_ids):\n",
    "        # Lookup embeddings.\n",
    "        user_embed = self.user_layer(user_ids)\n",
    "        pos_embed = self.item_layer(pos_item_ids)\n",
    "        neg_embed = self.item_layer(neg_item_ids)\n",
    "        # Concatenate user and positive item embeddings.\n",
    "        pos_input = torch.cat([user_embed, pos_embed], dim=1)\n",
    "        # Concatenate user and negative item embeddings.\n",
    "        neg_input = torch.cat([user_embed, neg_embed], dim=1)\n",
    "        # Compute matching scores.\n",
    "        pos_score = self.mlp(pos_input)\n",
    "        neg_score = self.mlp(neg_input)\n",
    "        # Compute the margin (triplet) loss.\n",
    "        loss = self.margin_loss(pos_score, neg_score)\n",
    "        return loss\n",
    "\n",
    "#########################################\n",
    "# DeepMatchModel for Evaluation\n",
    "#########################################\n",
    "class DeepMatchModel(nn.Module):\n",
    "    def __init__(self, user_layer, item_layer, mlp):\n",
    "        super().__init__()\n",
    "        # Uses the same learned user and item embedding layers and the MLP.\n",
    "        self.user_layer = user_layer\n",
    "        self.item_layer = item_layer\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Compute user-item matching scores for evaluation.\n",
    "        user_embed = self.user_layer(user_ids)\n",
    "        item_embed = self.item_layer(item_ids)\n",
    "        input_vec = torch.cat([user_embed, item_embed], dim=1)\n",
    "        return self.mlp(input_vec)\n",
    "\n",
    "#########################################\n",
    "# 3. Evaluation Function: Average ROC AUC\n",
    "#########################################\n",
    "def average_roc_auc(deep_match_model, pos_data_test, n_users, n_items):\n",
    "    \"\"\"\n",
    "    Computes the average ROC AUC across users.\n",
    "    For each user, scores for all items are generated and compared to the test set of positive items.\n",
    "    \"\"\"\n",
    "    all_auc = []\n",
    "    deep_match_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for user in range(n_users):\n",
    "            y_true = []\n",
    "            y_scores = []\n",
    "            for item in range(n_items):\n",
    "                score = deep_match_model(torch.LongTensor([user]), torch.LongTensor([item])).item()\n",
    "                y_scores.append(score)\n",
    "                # The label is 1 if the item is in the user's test positives.\n",
    "                label = 1 if item in pos_data_test.get(user, []) else 0\n",
    "                y_true.append(label)\n",
    "            # Skip users who have only one type of label.\n",
    "            if sum(y_true) == 0 or sum(y_true) == len(y_true):\n",
    "                continue\n",
    "            auc = roc_auc_score(y_true, y_scores)\n",
    "            all_auc.append(auc)\n",
    "    return np.mean(all_auc) if all_auc else 0.0\n",
    "\n",
    "#########################################\n",
    "# 4. Training Loop for Deep Recommender System\n",
    "#########################################\n",
    "def train_deep_recsys():\n",
    "    # Load the ML-100K data.\n",
    "    pos_data_train, pos_data_test, n_users, n_items = load_implicit_data()\n",
    "    \n",
    "    hyper_parameters = {\n",
    "        'user_dim': 32,\n",
    "        'item_dim': 64,\n",
    "        'n_hidden': 2,        # Increase the number of hidden layers\n",
    "        'hidden_size': 128,   # Increase the size of the hidden layer\n",
    "        'dropout': 0.1,\n",
    "        'l2_reg': 0.001,      # Reduce L2 regularization\n",
    "        'margin': 1.0         # Increase margin\n",
    "    }\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize the triplet-based model.\n",
    "    deep_triplet_model = DeepTripletModel(n_users, n_items, **hyper_parameters).to(device)\n",
    "    \n",
    "    # Create the DeepMatchModel for efficient evaluation.\n",
    "    deep_match_model = DeepMatchModel(deep_triplet_model.user_layer,\n",
    "                                      deep_triplet_model.item_layer,\n",
    "                                      deep_triplet_model.mlp).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(deep_triplet_model.parameters(), lr=0.001)\n",
    "    n_epochs = 200\n",
    "    batch_size = 128\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        deep_triplet_model.train()\n",
    "        # Sample a mini-batch of triplets.\n",
    "        user_ids, pos_item_ids, neg_item_ids = sample_triplets(pos_data_train, n_users, n_items, num_samples=batch_size)\n",
    "        user_ids = user_ids.to(device)\n",
    "        pos_item_ids = pos_item_ids.to(device)\n",
    "        neg_item_ids = neg_item_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = deep_triplet_model(user_ids, pos_item_ids, neg_item_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate using ROC AUC on test data.\n",
    "        auc = average_roc_auc(deep_match_model, pos_data_test, n_users, n_items)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}, Test ROC AUC: {auc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_deep_recsys()\n",
    "train_deep_recsys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
