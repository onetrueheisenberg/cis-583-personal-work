{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800db634-c6e8-4b98-ba4d-40608c56dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9218ee2-2a17-4cec-8bd2-cfffccbb646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Margin Loss ---\n",
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, pos_score, neg_score):\n",
    "        return torch.mean(F.relu(self.margin - pos_score + neg_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae97864-ace7-4ab8-8134-0c2d8853c351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Complete the MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, n_hidden=1, hidden_size=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        for _ in range(n_hidden - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb25a91-ab6a-4cbd-b8c3-d5a82f7b3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Complete the DeepTripletModel\n",
    "class DeepTripletModel(nn.Module):\n",
    "    def __init__(self, n_users, n_items, user_dim=64, item_dim=64, margin=1., \n",
    "                 n_hidden=1, hidden_size=64, dropout=0):\n",
    "        super().__init__()\n",
    "        self.user_layer = nn.Embedding(n_users, user_dim)\n",
    "        self.item_layer = nn.Embedding(n_items, item_dim)\n",
    "        self.mlp = MLP(input_dim=user_dim, n_hidden=n_hidden, hidden_size=hidden_size, dropout=dropout)\n",
    "        self.margin_loss = MarginLoss(margin)\n",
    "\n",
    "    def forward(self, user, item_pos, item_neg):\n",
    "        user_emb = F.normalize(self.user_layer(user), dim = 1)\n",
    "        item_pos_emb = F.normalize(self.item_layer(item_pos), dim = 1)\n",
    "        item_neg_emb = F.normalize(self.item_layer(item_neg), dim = 1)\n",
    "\n",
    "        pos_score = self.mlp(user_emb * item_pos_emb)\n",
    "        neg_score = self.mlp(user_emb * item_neg_emb)\n",
    "\n",
    "        # loss = self.margin_loss(pos_score, neg_score)\n",
    "        triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n",
    "        anchor = torch.randn(100, 128, requires_grad=True)\n",
    "        positive = torch.randn(100, 128, requires_grad=True)\n",
    "        negative = torch.randn(100, 128, requires_grad=True)\n",
    "        output = triplet_loss(anchor, pos_score, neg_score)\n",
    "        output.backward()\n",
    "        return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c962022-201b-48b4-a0a3-a299d786fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepMatchModel for Evaluation\n",
    "class DeepMatchModel(nn.Module):\n",
    "    def __init__(self, user_layer, item_layer, mlp):\n",
    "        super().__init__()\n",
    "        self.user_layer = user_layer\n",
    "        self.item_layer = item_layer\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.user_layer(user)\n",
    "        item_emb = self.item_layer(item)\n",
    "        return self.mlp(user_emb * item_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3097226-5cce-49cf-9530-1bff0da29265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Complete the evaluation function\n",
    "def manual_roc_auc(y_true, y_score):\n",
    "    # Sort by predicted score\n",
    "    desc_score_indices = np.argsort(-y_score)\n",
    "    y_true = np.array(y_true)[desc_score_indices]\n",
    "    y_score = np.array(y_score)[desc_score_indices]\n",
    "\n",
    "    pos_count = np.sum(y_true)\n",
    "    neg_count = len(y_true) - pos_count\n",
    "\n",
    "    if pos_count == 0 or neg_count == 0:\n",
    "        return None  # Undefined AUC\n",
    "\n",
    "    cumulative_pos = np.cumsum(y_true)\n",
    "    cumulative_neg = np.cumsum(1 - y_true)\n",
    "\n",
    "    tpr = cumulative_pos / pos_count\n",
    "    fpr = cumulative_neg / neg_count\n",
    "\n",
    "    # Calculate AUC using trapezoidal rule\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc\n",
    "def manual_average_roc_auc(model, test_data, n_items):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    for user in test_data:\n",
    "        pos_items = test_data[user]\n",
    "        if not pos_items:\n",
    "            continue\n",
    "\n",
    "        all_items = list(range(n_items))\n",
    "        user_tensor = torch.tensor([user] * len(all_items))\n",
    "        item_tensor = torch.tensor(all_items)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(user_tensor, item_tensor).cpu().numpy()\n",
    "\n",
    "        labels = np.isin(all_items, pos_items).astype(int)\n",
    "\n",
    "        if len(set(labels)) < 2:\n",
    "            continue  # Skip users with all 1s or all 0s\n",
    "\n",
    "        auc = manual_roc_auc(labels, predictions)\n",
    "        if auc is not None:\n",
    "            scores.append(auc)\n",
    "\n",
    "    return np.mean(scores) if scores else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39fb63f6-c969-4bf9-bffe-74e3be9cfdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Implicit Data\n",
    "def load_implicit_data():\n",
    "    df = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    df = df.drop(columns=['timestamp'])\n",
    "    df['user_id'] -= 1\n",
    "    df['item_id'] -= 1\n",
    "\n",
    "    # Train/test split by interaction\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    def build_dict(df):\n",
    "        data = {}\n",
    "        for row in df.itertuples():\n",
    "            data.setdefault(row.user_id, []).append(row.item_id)\n",
    "        return data\n",
    "\n",
    "    return build_dict(train_df), build_dict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81cb4d66-5fb9-4cf8-a644-05cb978b1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Implement the training loop\n",
    "def train_deep_recsys():\n",
    "    train_data, test_data = load_implicit_data()\n",
    "    n_users = max(max(train_data.keys()), max(test_data.keys())) + 1\n",
    "    n_items = max(max([max(v) for v in train_data.values()]), max([max(v) for v in test_data.values()])) + 1\n",
    "\n",
    "    model = DeepTripletModel(\n",
    "        n_users=n_users,\n",
    "        n_items=n_items,\n",
    "        user_dim=32,\n",
    "        item_dim=32,\n",
    "        margin=2.0,\n",
    "        n_hidden=3,\n",
    "        hidden_size=128,\n",
    "        dropout=0.3\n",
    "    )\n",
    "\n",
    "    match_model = DeepMatchModel(model.user_layer, model.item_layer, model.mlp)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5) # with l2 regularization\n",
    "\n",
    "    def sample_triplets(data):\n",
    "        users, pos_items, neg_items = [], [], []\n",
    "        for user, items in data.items():\n",
    "            for _ in range(len(items)):\n",
    "                pos = np.random.choice(items)\n",
    "                # neg = np.random.randint(0, n_items)\n",
    "                # while neg in items:\n",
    "                #     neg = np.random.randint(0, n_items)\n",
    "                neg_candidates = [np.random.randint(0, n_items) for _ in range(20)]\n",
    "                neg_scores = [model.mlp(model.user_layer(torch.tensor([user])) * model.item_layer(torch.tensor([neg]))) for neg in neg_candidates]\n",
    "                neg = neg_candidates[np.argmax([score.item() for score in neg_scores])]\n",
    "                users.append(user)\n",
    "                pos_items.append(pos)\n",
    "                neg_items.append(neg)\n",
    "        return torch.tensor(users), torch.tensor(pos_items), torch.tensor(neg_items)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        user, pos, neg = sample_triplets(train_data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(user, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        auc = manual_average_roc_auc(match_model, test_data, n_items)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}, ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b53d75b-1439-409f-a0be-e802a1b85233",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The anchor, positive, and negative tensors are expected to have the same number of dimensions, but got: anchor 2D, positive 1D, and negative 1D inputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_deep_recsys()\n",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mtrain_deep_recsys\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m user, pos, neg \u001b[38;5;241m=\u001b[39m sample_triplets(train_data)\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(user, pos, neg)\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mDeepTripletModel.forward\u001b[0;34m(self, user, item_pos, item_neg)\u001b[0m\n\u001b[1;32m     22\u001b[0m positive \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m128\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m negative \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m128\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m triplet_loss(anchor, pos_score, neg_score)\n\u001b[1;32m     25\u001b[0m output\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m triplet_loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:1683\u001b[0m, in \u001b[0;36mTripletMarginLoss.forward\u001b[0;34m(self, anchor, positive, negative)\u001b[0m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, anchor: Tensor, positive: Tensor, negative: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mtriplet_margin_loss(\n\u001b[1;32m   1684\u001b[0m         anchor,\n\u001b[1;32m   1685\u001b[0m         positive,\n\u001b[1;32m   1686\u001b[0m         negative,\n\u001b[1;32m   1687\u001b[0m         margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin,\n\u001b[1;32m   1688\u001b[0m         p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp,\n\u001b[1;32m   1689\u001b[0m         eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m   1690\u001b[0m         swap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswap,\n\u001b[1;32m   1691\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1692\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:5261\u001b[0m, in \u001b[0;36mtriplet_margin_loss\u001b[0;34m(anchor, positive, negative, margin, p, eps, swap, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   5259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m margin \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmargin must be greater than 0, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmargin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 5261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtriplet_margin_loss(\n\u001b[1;32m   5262\u001b[0m     anchor, positive, negative, margin, p, eps, swap, reduction_enum\n\u001b[1;32m   5263\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The anchor, positive, and negative tensors are expected to have the same number of dimensions, but got: anchor 2D, positive 1D, and negative 1D inputs"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "train_deep_recsys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
