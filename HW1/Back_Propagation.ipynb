{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e55ca37b-5d65-4660-ae58-ab656eff54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(3) # To make repeatable\n",
    "LEARNING_RATE = 0.1\n",
    "index_list = [0, 1, 2, 3] # Used to randomize order\n",
    "\n",
    "# Define training examples.\n",
    "x_train = [np.array([1.0, -1.0, -1.0]),\n",
    "           np.array([1.0, -1.0, 1.0]),\n",
    "           np.array([1.0, 1.0, -1.0]),\n",
    "           np.array([1.0, 1.0, 1.0])]\n",
    "y_train = [0.0, 1.0, 1.0, 0.0] # Output (ground truth)\n",
    "\n",
    "def neuron_w(input_count):\n",
    "    weights = np.zeros(input_count+1)\n",
    "    for i in range(1, (input_count+1)):\n",
    "        weights[i] = np.random.uniform(-1.0, 1.0)\n",
    "    return weights\n",
    "\n",
    "n_w = [neuron_w(2), neuron_w(2), neuron_w(2)]\n",
    "n_y = [0, 0, 0]\n",
    "n_error = [0, 0, 0]\n",
    "\n",
    "# def show_learning():\n",
    "    # print('Current weights:')\n",
    "    # for i, w in enumerate(n_w):\n",
    "        # print('neuron ', i, ': w0 =', '%5.2f' % w[0],\n",
    "        #       ', w1 =', '%5.2f' % w[1], ', w2 =',\n",
    "        #       '%5.2f' % w[2])\n",
    "    # print('----------------')\n",
    "\n",
    "def forward_pass(x):\n",
    "    global n_y\n",
    "    n_y[0] = np.tanh(np.dot(n_w[0], x)) # Neuron 0\n",
    "    n_y[1] = np.tanh(np.dot(n_w[1], x)) # Neuron 1\n",
    "    n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # 1.0 is bias\n",
    "    z2 = np.dot(n_w[2], n2_inputs)\n",
    "    n_y[2] = 1.0 / (1.0 + np.exp(-z2))\n",
    "\n",
    "def backward_pass(y_truth):\n",
    "    global n_error\n",
    "    error_prime = -(y_truth - n_y[2]) # Derivative of loss-func\n",
    "    derivative = n_y[2] * (1.0 - n_y[2]) # Logistic derivative\n",
    "    n_error[2] = error_prime * derivative\n",
    "    derivative = 1.0 - n_y[0]**2 # tanh derivative\n",
    "    n_error[0] = n_w[2][1] * n_error[2] * derivative\n",
    "    derivative = 1.0 - n_y[1]**2 # tanh derivative\n",
    "    n_error[1] = n_w[2][2] * n_error[2] * derivative\n",
    "\n",
    "def adjust_weights(x):\n",
    "    global n_w\n",
    "    n_w[0] -= (x * LEARNING_RATE * n_error[0])\n",
    "    n_w[1] -= (x * LEARNING_RATE * n_error[1])\n",
    "    n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # 1.0 is bias\n",
    "    n_w[2] -= (n2_inputs * LEARNING_RATE * n_error[2])\n",
    "    # Network training loop.\n",
    "all_correct = False\n",
    "while not all_correct: # Train until converged\n",
    "    all_correct = True\n",
    "    np.random.shuffle(index_list) # Randomize order\n",
    "    for i in index_list: # Train on all examples\n",
    "        forward_pass(x_train[i])\n",
    "        backward_pass(y_train[i])\n",
    "        adjust_weights(x_train[i])\n",
    "        # show_learning() # Show updated weights\n",
    "    for i in range(len(x_train)): # Check if converged\n",
    "        forward_pass(x_train[i])\n",
    "        # print('x1 =', '%4.1f' % x_train[i][1], ', x2 =',\n",
    "        #       '%4.1f' % x_train[i][2], ', y =',\n",
    "        #       '%.4f' % n_y[2])\n",
    "        if(((y_train[i] < 0.5) and (n_y[2] >= 0.5))\n",
    "                or ((y_train[i] >= 0.5) and (n_y[2] < 0.5))):\n",
    "            all_correct = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bdc9d2-c243-4916-b95a-8b4400561670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
